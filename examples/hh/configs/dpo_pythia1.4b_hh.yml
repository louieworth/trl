model_name_or_path: EleutherAI/pythia-1.4b
dataset_name: trl-internal-testing/hh-rlhf-trl-style
per_device_train_batch_size: 16
learning_rate: 1e-6
gradient_accumulation_steps: 4
logging_steps: 100
eval_steps: 500
num_train_epochs: 20
evaluation_strategy: "steps"
output_dir: "dpo_anthropic_hh"
optim: rmsprop
warmup_steps: 150
report_to: wandb
bf16: True 
logging_first_step: True
no_remove_unused_columns: True
use_peft: True
lora_r: 16
lora_alpha: 16
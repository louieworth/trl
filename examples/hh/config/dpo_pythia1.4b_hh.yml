model_name_or_path: EleutherAI/pythia-1.4b
dataset_name: Anthropic/hh-rlhf
per_device_train_batch_size: 4
learning_rate: 1e-3
gradient_accumulation_steps: 1
logging_steps: 10
eval_steps: 500
output_dir: "dpo_anthropic_hh"
optim: rmsprop
warmup_steps: 150
report_to: wandb
bf16: True 
logging_first_step: True
no_remove_unused_columns: True
use_peft: True
lora_r: 16
lora_alpha: 16